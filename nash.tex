%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


%% \documentclass{sigplanconf}
%% \documentclass[preprint]{sigplanconf}
\documentclass[preprint, 10pt]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% numbers       To obtain numeric citation style instead of author/year.

\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{multirow}
\usepackage[T1]{fontenc}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=violet
}

\lstset{
  basicstyle=\footnotesize,
  numbers=left
}

\newcommand{\cL}{{\cal L}}

\fvset{commandchars=\\\{\}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{20yy}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\copyrightdoi{nnnnnnn.nnnnnnn}

% Uncomment the publication rights you want to use.
%\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

% These are ignored unless 'preprint' option specified.
\titlebanner{DRAFT}

\preprintfooter{DRAFT --- Nash: a tracing JIT VM for Guile}

\title{Nash: A Tracing JIT VM For Guile}
%% \subtitle{Subtitle Text, if any}

\authorinfo{Atsuro Hoshino}
           {}
           {hoshinoatsuro@gmail.com}
%% \authorinfo{Name2\and Name3}
%%            {Affiliation2/3}
%%            {Email2/3}

\maketitle

\begin{abstract}

This paper introduces \textit{Nash}, an experimental \textit{virtual machine}
(VM) for GNU Guile with tracing \textit{just-in-time} (JIT) compiler. Nash is
designed as a drop-in replacement for Guile's existing VM.\@ Nash could be
used for running scripts, used at a REPL, and embedded in other
programs. Design of Nash internals is discussed, including its VM interpreter
which records frequently executed instructions found in Guile's bytecode, and
its JIT compiler which emits native code from recorded instructions. Nash
coexists with Guile's existing VM.\@ Lots of Guile's features, such as
bytecode interpreter, are reused in Nash. When conditions were met, Nash runs
more than \textit{$40\times$ faster} than Guile's existing VM, without
modifying the input program. Benchmark results of Nash are shown, including
comparisons with other Scheme implementations.

\end{abstract}

%% \category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%% \terms
%% term1, term2

\keywords{} Just-In-Time Compilation, Virtual~Machine, Implementation,
Scheme~Programming~Language

\section{Introduction}

From its simple design, Scheme is used for various purposes, many
implementations exist. One of the uses is as an extension language embedded in
other program. Implementations such as Chibi-Scheme and TinyScheme are
designed with use as an extension language in mind. On the other hand, there
are Scheme implementations used for more expensive computations. This kind of
Scheme implementations typically compiles to native code before executing. The
compilation could be done \textit{ahead-of-time} (AOT), such as in
Bigloo~\cite{serrano1995bigloo} and Gambit~\cite{feeley1998gambit}, or
incrementally, such as in Chez~\cite{dybvig2006development} and
Larceny~\cite{hansen1992impact}, or in a mixture of AOT and JIT compilation,
such as in Pycket~\cite{bauman2015pycket}, and Racket~\cite{flatt2013racket}.

There exist a performance gap between Scheme implementations which does native
code compilation and which doesn't. Tracing JIT compilation is a technique
used in VM to improve performance by compiling the frequently executed
instruction code paths. Dynamo~\cite{bala2000dynamo} has pioneered the use of
tracing JIT by tracing native code. Later the technique was used in various
VMs for dynamic programming language to achieve performance
improvement. Languages such as Lua~\cite{pall2016luajit},
JavaScript~\cite{gal2009trace}, and Python~\cite{bolz2009tracing} have made
success with VMs which implement tracing JIT.\@

\textit{Nash} is a new experimental tracing JIT VM for GNU Guile. Guile is a
general-purpose Scheme implementation which could be used as an extension
language, as a scripting engine, and for application development. Guile offers
\textit{libguile} to allow itself to be embedded in other program. GnuCash,
gEDA, GNU Make, and GDB uses Guile as an extension language. Guile implements
standard R5RS~\cite{abelson1998revised5}, most of
R6RS~\cite{sperber2010revised}, several SRFIs, and many extensions of its own,
including delimited continuations and native POSIX thread
support~\cite{Galassi02guilereference}. Nash is designed to be a drop-in
replacement for Guile's existing VM, which is called \textit{VM-regular} in
this paper, to achieve performance improvement.

\begin{figure}
  \begin{center}
    \small
\begin{verbatim}
1 (define (sumv-positive vec)
2   (let lp ((i 0) (acc 0))
3     (if (<= (vector-length vec) i)
4         acc
5         (lp (+ i 1)
6             (if (< 0 (vector-ref vec i))
7                 (+ acc (vector-ref vec i))
8                 acc)))))
\end{verbatim}
\end{center}
\caption{Scheme source code of sample procedure.}
\label{fig:scmloop}
\end{figure}

Figure~\hyperref[fig:scmloop]{\ref{fig:scmloop}} shows Scheme source code of a
sample procedure \texttt{sumv-positive} which contains a loop. Details of Nash
internal are explained with using \texttt{sumv-positive}. The
\texttt{sumv-positive} procedure takes a single argument \texttt{vec}, a
vector containing numbers. The loop inside the procedure checks whether the
\texttt{i}-th \texttt{vector-ref} of \texttt{vec} is greater than \texttt{0},
adds up the element if true. The loop repeat the comparison and addition with
incremented \texttt{i} until \texttt{i} is greater than the
\texttt{vector-length} of \texttt{vec}.

Section~\hyperref[sec:background]{\ref{sec:background}} briefly mentions some
background of Guile. In Guile, Scheme source codes are compiled to bytecode
before the execution.\footnote{By default, source codes are byte-compiled
  before executed. Guile can run Scheme source code without compilation so
  that trivial computations could be done quickly.} When Nash executes
\texttt{sumv-positive}, the computation starts with bytecode
interpretation. After executing the bytecode for a while, the bytecode
interpreter detects a hot loop in the body of \texttt{sumv-positive}. Then the
bytecode interpreter switches its state, start recording the bytecode
instructions and corresponding stack values of the loop. Recording of
instructions are described in
Section~\hyperref[sec:interpreter]{\ref{sec:interpreter}}. When the bytecode
interpreter reached to the beginning of the observed loop, recorded data are
passed to JIT compiler. The JIT compiler is written in Scheme, executed with
VM-regular. The compiler uses recorded bytecode and stack values to emit
optimized native code. The variables from the stack are used to specify types,
possibly emitting \textit{guards} to exit from the compiled native code. More
detail of the JIT compiler internals are covered in
Section~\hyperref[sec:compiler]{\ref{sec:compiler}}.

The rest of the sections are organized as follows.
Section~\hyperref[sec:evaluation]{\ref{sec:evaluation}} shows results from
benchmark, including comparisons between Nash and other Scheme
implementations. Section~\hyperref[sec:conclusion]{\ref{sec:future}} discusses
current limitations and possibilities for future work. Finally,
Section~\hyperref[sec:related]{\ref{sec:related}} mentions related works, and
Section~\hyperref[sec:conclusion]{\ref{sec:conclusion}} concludes this paper.

\section{Background}
\label{sec:background}

This section describes background information and brief history of tracing JIT
and GNU Guile. Some of the Guile internals affecting the design of Nash are
mentioned.

\subsection{Tracing JIT}
Tracing JIT is one of JIT compilation styles~\cite{bolz2009tracing} which
assumes that:

\begin{itemize}
\item Programs spend most of their runtime in loops.
\item Several iterations of the same loop are likely to take similar code
  paths.
\end{itemize}

After Dynamo~\cite{bala2000dynamo} used the technique to trace native code,
various development has been done in the area. LuaJIT is one of the successful
implementation of tracing JIT VM for the Lua~\cite{ierusalimschy1996lua}
programming language. Pypy~\cite{bolz2009tracing} is a tracing JIT VM for the
Python programming language. Pypy is implemented with RPython framework, which
is a \textit{meta-tracing} infrastructure to develop a tracing JIT VM by
defining an interpreter of the target language. The framework was adapted to
other language than Python, including Pycket~\citep{bauman2015pycket}, a
tracing JIT VM for Racket, and Pixie, a tracing JIT VM for the Pixie language,
which is a dialect of Lisp.

Typical settings for tracing JIT of dynamic programming language contains an
interpreter and a JIT compiler. The interpreter observes the execution of
instructions, detects hot loops, and records frequently executed
instructions. The recorded instructions are often called \textit{trace}. JIT
compiler then compiles the trace to get optimized native code of the hot
loop. The interpreter typically has a functionality to switch between a phase
for observing the loop, a phase for recording the instructions, and a phase
for executing the compiled native code.  Compiled native code contains
\textit{guards} to terminate the execution of native code, and bring the
control of program back to the interpreter. Guards are inserted when a trace
contained conditions which might not satisfied in later iteration of the
loop. For instance, the loop in
Figure~\hyperref[fig:scmloop]{\ref{fig:scmloop}} will emit a guard which
compares the value of \texttt{i} with the length of the vector. In dynamic
programming languages such as Scheme, guards for type check may be inserted as
well, since compiler could generate more optimized native code when the types
of the values are known at compilation time.

\subsection{GNU Guile}
\label{sec:backgroundguile}

GNU Guile was born to be an official extension language for GNU
projects~\cite{Galassi02guilereference}. Since then, various developers have
made changes to the implementation. As of version 2.1.2, Guile contains a
bytecode compiler and a VM which interprets the compiled bytecode. Guile uses
conservative Boehm-Demers-Weiser garbage collector~\cite{boehm1988garbage}.

\subsubsection{SCM Data Type}
\label{sec:scmdatatype}

\begin{table}
  \begin{center}
  \begin{tabular}{ccl}
    Tag&Type&Scheme value\\
    \toprule
    \texttt{xxxxxxxxxxxxx000} & heap object & `foo, \#(1 2 3) \ldots \\
    \texttt{xxxxxxxxxxxxxx10} & small integer & 1, 2, 3, 4, 5 \ldots \\
    \texttt{xxxxxxxxxxxxx100} & boolean false & \#f \\
    \texttt{xxxxxxxx00001100} & character & \#\textbackslash{} a,
    \#\textbackslash{} b, \ldots \\
    \texttt{xxxxxx1100000100} & empty list & `() \\
    \texttt{xxxxx10000000100} & boolean true & \#t \\
  \end{tabular}
  \end{center}
  \caption{Tag value with corresponding Scheme type and Scheme value. The
    ``\texttt{x}'' in the tag column indicates any value.}
\label{tab:tags}
\end{table}

Guile's internal data type for Scheme object is defined as typedef
\texttt{SCM} in C~\cite{Galassi02guilereference}. \texttt{SCM} value contains
a type tag to identify its type in Scheme, which could be categorized as
\textit{immediates} or \textit{heap objects}. Immediates consists of a
combination of type tag and the value to identify itself. Immediates includes
booleans, characters, small integers, the empty list, the end of file object,
the \textit{unspecified} object, \textit{nil} object used in the Emacs-Lisp
compatibility mode, and other special objects used internally. Heap objects
are all the other types which could not fit itself in the bit size of
\texttt{SCM}, such as symbols, lists, vectors, strings, procedures,
multi-precision integer numbers, and so on.  When Guile decide the type of
\texttt{SCM} value, firstly the three least significant bits (called
\textit{tc3} tag in Guile) of \texttt{SCM} value is checked to see whether the
value belongs to immediates or heap objects. Table
\hyperref[tab:tags]{\ref{tab:tags}} shows the type tags for immediates and
heap objects. When tc3 tag was \texttt{000}, the value belongs to heap
objects. All the other values of tc3 tag are immediates, though some of the
tag values are unused. For instance, tc3 tag \texttt{010} and \texttt{110} are
used for small integers, the first two bits are used for a tag and the rest of
the bits are used for an integer value. Scheme value $0$ is SCM $00000010$,
Scheme value $1$ is SCM $00000110$, Scheme value 2 is SCM $00001010$, and so
on. Types of various heap objects are decided by non-tc3 tag part of
\texttt{SCM} value. In such case, the non-tc3 tag part contains extra tag
values and an address pointing to the contents of the heap object.

\subsubsection{Bytecode Compiler}

\begin{figure}
  \begin{center}
    \small
\begin{verbatim}
#<sumv-positive (vec)> at #x7f4ddb7fc51c:

   0    (assert-nargs-ee/locals 2 5)
        ...
L2:
  22    (uadd/immediate 0 6 1)
  23    (vector-ref 6 5 6)
  24    (br-if-u64-<-scm 3 6 #t 4)      ;; -> L3
  27    (add 1 1 6)
L3:
  28    (br-if-u64-<= 4 0 #f 9)         ;; -> L6
  31    (mov 6 0)
  32    (br -10)                        ;; -> L2
        ...
L6:
  37    (mov 5 1)
  38    (return-values 2)
\end{verbatim}
\end{center}
\caption{Byte compiled code of \texttt{sumv-positive}. The contents is
  slightly modified from output of disassembler for displaying purpose and
  simplicity.}
\label{fig:bytecode}
\end{figure}

Guile's bytecode compiler is designed as \textit{compiler tower}. The compiler
consists of several compilers defining tower of languages. Each step of the
compilation sequence knows how to compile down to the step below, until the
compiled output turns into bytecode instructions executed by the VM.\@

In Guile version 2.1.2, Scheme input programs are first translated to a
program in \textit{tree-il} language, an internal representation used by
Guile. Then resulting tree-il program is compiled to \textit{cps}, which is
another internal representation. Then the resulting cps code is translated to
bytecode instructions. Guile contains compilers for Emacs-Lisp to tree-il, and
Ecmascript to tree-il. The resulting tree-il language made out from Emacs-Lisp
and Ecmascript reuse the rest of compilation path to bytecode
instructions. The definition of the compilation steps could be modified, which
helps the user to add a new high-level language compiled to \texttt{tree-il},
or directly compiling to bytecode, or compiling to different new
target. Guile's bytecode compiler applies various optimizations, with all of
them turned on by default. The optimizations could be turned off to save
compilation time by sacrificing run time performances.

Figure~\hyperref[fig:bytecode]{\ref{fig:bytecode}} shows compiled bytecode of
\texttt{sumv-positive}. Each bytecode instruction takes arguments and its use
varies, some arguments are used as constant, some are used as an index value
to read or write a value in current stack. The first line of the figure shows
the procedure name and memory address of the byte-compiled data.  The numbers
shown in the left of each line are bytecode \textit{instruction pointer} (IP)
offset. For IP offsets specified as a jump destination, a numbered label
starting from \texttt{L} is shown (IP offset 22, 28, and 37 in the
figure). The bytecode which may cause a jump contains a comment with the
destination label (IP offset 24, 28, and 32 in the figure).

When \texttt{sumv-positive} was called, VM-regular executes the bytecode from
IP offset 0. Later the execution reaches to IP offset 32, \texttt{(br
  -10)}. The \texttt{br} bytecode instruction performs an unconditional jump
by adding its argument to current IP.\@ Negative offset means a backward jump,
which is the case shown in the figure. After the jump, current IP offset is 22
(labeled as \texttt{L2} in the figure), which indicates a loop. In the
bytecode instructions inside the loop, IP offset 24 and 28 are branching
instructions. IP offset 24 may skip \texttt{(add 1 1 6)} instruction at IP
offset 27. IP offset 28 may jump to \texttt{L6} to exit from the loop and
return the value to the caller of \texttt{sumv-positive}.

\begin{figure}
  \centering
  \includegraphics[width=0.45 \textwidth]{overview}
  \caption{A flowchart diagram showing program execution in Nash.}
\label{fig:overview}
\end{figure}

\section{Nash Interpreter}
\label{sec:interpreter}

\subsection{Nash Overview}
\label{sec:overview}

Nash is designed as a drop-in replacement for VM-regular, could be used to run
scripts, has REPL, and could be embedded in a C program as an extension
language. Figure~\hyperref[fig:overview]{\ref{fig:overview}} shows flowchart
of program execution in Nash, clustered by the software component in
subject.

Typical control flow works as follows.  \textbf{1.} Nash starts executing
bytecode of Scheme program from interpreter. The interpreter evaluates
bytecode instructions and keeps track of loops. \textbf{2.} The interpreter
detects a hot loop. Then the interpreter looks up for native code. If native
code was not found, the interpreter continues the evaluation of instructions
with recording the bytecode and the values from current stack.  \textbf{3.}
The bytecode IP reaches to the end of the loop. The recording ends and traced
information are passed to Nash compiler. \textbf{4.} Compilation finishes with
native code generation. Control flow gets back to the interpreter. Bytecode
interpretation continues from the IP where the recording has
ended. \textbf{5.} The interpreter encounters the bytecode IP of the hot loop
again. This time compiled native code exists for the IP.\@ The interpreter
executes the native code.  \textbf{6.} A guard in the native code
fails. Native code goes through a bailout code to recover the state of the
interpreter and control flow gets back to the interpreter. \textbf{7.}
Interpreter reaches to the end of input bytecode, and program terminates.
Transition \textbf{2} and \textbf{5} are repeated as necessary.

\subsection{VM Engine For Nash}

Guile uses C functions to interpret compiled bytecode. This C function is
called \textit{VM engine}. VM engine is defined in a dedicated file named
\texttt{vm-engine.c}, which is included multiple times from other source
code. This design enables defining multiple VM engine in few lines of C
codes. As of Guile 2.1.2, there are two VM engines, one named
\texttt{vm\_regular\_engine} which is used by VM-regular to interpret
bytecode, and another named \texttt{vm\_debug\_engine} which is used for
debugging. A single function named \texttt{VM\_NAME} is defined in
\texttt{vm-engine.c}.  When including \texttt{vm-engine.c}, \texttt{VM\_NAME}
is defined with unique literal. The following C codes are written in other
file than \texttt{vm-engine.c} to define \texttt{vm\_regular\_engine} and
\texttt{vm\_debug\_engine}:

\begin{verbatim}
  #define VM_NAME vm_regular_engine
  #include "vm-engine.c"
  #undef VM_NAME
  ...
  #define VM_NAME vm_debug_engine
  #include "vm-engine.c"
  #undef VM_NAME
\end{verbatim}

Inside the \texttt{VM\_NAME} function, each bytecode instructions is defined
by \texttt{VM\_DEFINE\_OP} macro with unique instruction number, with C macro
\texttt{NEXT} at the last of definition body to perform next
instruction. \texttt{VM\_DEFINE\_OP} macro fills in the jump table used by
\texttt{VM\_NAME} to define jump destinations.\footnote{Strictly speaking,
  Guile chooses jump table or \texttt{switch \ldots\@ case} expression at
  build time for dispatching bytecode, by deciding whether the platform
  supports label as values (computed \texttt{goto}).}  Some of the
instructions continue the interpretation with \texttt{NEXT} using constant
offset value. For instance, \texttt{uadd\_immediate} instruction always
progress to the bytecode stored at $1$ byte next from the current IP, thus the
\texttt{NEXT} macro takes constant value $1$. Some instructions use offset
value specified from argument, such as \texttt{(br -10)} shown in
Figure~\hyperref[fig:bytecode]{\ref{fig:bytecode}}.  The bytecode interpreter
in Nash uses the same \texttt{vm-engine.c} file. The file is included once
more with following codes:

\begin{verbatim}
  #define VM_NAME vm_nash_engine
  #define VM_NASH 1
  #include "vm-engine.c"
  #undef VM_NAME
\end{verbatim}

Few small modifications were made to \texttt{vm-engine.c}. Nash uses C macros
to add tracing functionality to \texttt{VM\_NAME} function. C macros
\texttt{VM\_NASH\_CALL}, \texttt{VM\_NASH\_TAILCALL}, and
\texttt{VM\_NASH\_JUMP} were added to detect hot loops and enter compiled
native code, and \texttt{VM\_NASH\_MERGE} to record instructions.

\begin{figure}
  \centering
  \small
\begin{Verbatim}
 static SCM
 VM_NAME (scm_i_thread *thread, struct scm_vm *vp,
          scm_i_jmp_buf *registers, int resume) \{
   ...
   VM_DEFINE_OP (1, call, ...) \{
       ...
\textcolor{OrangeRed}{-}      \textcolor{OrangeRed}{NEXT(offset);}
\textcolor{OliveGreen}{+}      \textcolor{OliveGreen}{VM_NASH_CALL (old_ip);}
   \}
   VM_DEFINE_OP (3, tail_call ...) \{
       ...
\textcolor{OrangeRed}{-}      \textcolor{OrangeRed}{NEXT(offset);}
\textcolor{OliveGreen}{+}      \textcolor{OliveGreen}{VM_NASH_TAIL_CALL (old_ip);}
   \}
   VM_DEFINE_OP (33, br, ...) \{
       ...
\textcolor{OrangeRed}{-}      \textcolor{OrangeRed}{NEXT(offset);}
\textcolor{OliveGreen}{+}      \textcolor{OliveGreen}{VM_NASH_JUMP (offset);}
   \}
   VM_DEFINE_OP (152, uadd_immediate, ...) \{
       ...
       NEXT (1);
   \}
   ...
 \}
\end{Verbatim}
\caption{Modified \texttt{VM\_NAME} function. Lines starting with \texttt{-}
  were deleted, \texttt{+} were added for Nash.}
\label{fig:vmnamenash}
\end{figure}

\subsubsection{Finding Loops}

Figure~\hyperref[fig:vmnamenash]{\ref{fig:vmnamenash}} shows a snippet
containing modifications made to \texttt{VM\_NAME}. \texttt{VM\_NASH\_JUMP} is
in the definition body of \texttt{br}, \texttt{VM\_NASH\_CALL} in
\texttt{call}, and \texttt{VM\_NASH\_TAIL\_CALL} in
\texttt{tail-call}. Bytecode definitions \texttt{br}, \texttt{call}, and
\texttt{tail-call} were marked since these bytecode perform jumps to start
loops.\footnote{Guile has more bytecode instructions for branching, such as
  \texttt{br-if-u64-<=}. These branching instructions are marked with
  \texttt{VM\_NASH\_JUMP} since they may perform a backward jump, though not
  shown in the figure.}  Bytecode definitions which do not start a loop, such
as \texttt{uadd\_immediate}, are unmodified. The definition of \texttt{br}
contains \texttt{VM\_NASH\_JUMP} with a parameter \texttt{offset}. When the
\texttt{offset} was negative, the backward jump will be detected as a loop by
Nash. When \texttt{br} with negative offset was found in interpreted bytecode,
\texttt{VM\_NASH\_JUMP} looks for a native code with the next IP.\@ If a
native code was found, the native code is executed. Otherwise,
\texttt{VM\_NASH\_JUMP} increment the counter value for the IP, and if the
counter value exceeds a threshold parameter, \texttt{vm\_nash\_engine} starts
recording the bytecode instruction in current loop.  Similarly,
\texttt{VM\_NASH\_CALL} and \texttt{VM\_NASH\_TAIL\_CALL} use its argument
\texttt{old\_ip} to detect loops from consequent calls and tail-calls,
respectively.\@

The internal C codes of \texttt{VM\_NASH\_JUMP}, \texttt{VM\_NASH\_CALL}, and
\texttt{VM\_NASH\_TCALL} are mostly shared. One of the differences between the
three is to use different strategy to decide hot loops by using different
values to increment the loop counter. For instance, \texttt{VM\_NASH\_JUMP}
may add $2$ to the loop counter, while \texttt{VM\_NASH\_CALL} may add $1$,
which will result in a setting that backward jumps get hot sooner than
consequent calls.  \texttt{VM\_NASH\_JUMP}, \texttt{VM\_NASH\_CALL}, and
\texttt{VM\_NASH\_TAIL\_CALL} are defined as \texttt{NEXT} when including the
file \texttt{vm-engine.c} to define other VM engines.

\subsubsection{Recording Instructions}

Figure~\hyperref[fig:cnext]{\ref{fig:cnext}} shows the modified contents of
\texttt{NEXT} in \texttt{VM\_NAME} function. When \texttt{vm\_nash\_engine}
found a loop, observed bytecode and stack values are recorded by
\texttt{VM\_NASH\_MERGE}. Figure~\hyperref[fig:cnext]{\ref{fig:cnext}} shows
how the interpretation continues with updating the value of \texttt{ip}, which
is a variable in \texttt{VM\_NAME} used for bytecode IP.\@ When the value of
\texttt{ip} matched the beginning of the loop, \texttt{VM\_NASH\_MERGE} will
stop recording, and pass the recorded data to JIT
compiler. Figure~\hyperref[fig:trace]{\ref{fig:trace}} shows dumped sample
data of recorded bytecode and stack values made by running
\texttt{sumv-positive} with a length 1000 \texttt{vector} argument containing
random small integer numbers from \texttt{-10} to \texttt{10}. The hexadecimal
numbers in left are the absolute bytecode IP of each bytecode instruction, and
the commented out vector in each line contains \texttt{SCM} representation of
the values in the stack at the time of recording.  The first bytecode
\texttt{(uadd/immediate 0 6 1)} in
Figure~\hyperref[fig:trace]{\ref{fig:trace}} is shown at IP offset 22 in
Figure~\hyperref[fig:bytecode]{\ref{fig:bytecode}}. \texttt{vm\_nash\_engine}
continued the recording with IP offset 23, 24, 27, 28, and 31. Then at IP
offset 32, bytecode \texttt{(br -10)} will be recorded and the interpreter
jumps back to IP offset 22, which was labeled as \texttt{L2} in
Figure~\hyperref[fig:bytecode]{\ref{fig:bytecode}}. The bytecode IP matches
the IP where the recording has started, \texttt{vm\_nash\_engine} stops the
recording. \texttt{VM\_NASH\_MERGE} is defined with empty body when including
\texttt{vm-engine.c} file for other VM engines.

\begin{figure}
  \centering
  \small
\begin{Verbatim}
 # define NEXT(n)                            \textbackslash
   do \{                                      \textbackslash
       ip += n;                              \textbackslash
\textcolor{OliveGreen}{+}      \textcolor{OliveGreen}{VM_NASH_MERGE ();                     \textbackslash}
       ...
       op = *ip;                             \textbackslash
       goto *jump_table[op & 0xff];          \textbackslash
   \} while (0)
\end{Verbatim}
\caption{Modified definition of \texttt{NEXT}. Line starting with \texttt{+} was
  added for Nash.}
\label{fig:cnext}
\end{figure}

\section{Nash Compiler}
\label{sec:compiler}

This section describes the details of JIT compiler in Nash, which is written
in Scheme. Compiled bytecode of the JIT compiler is executed by
\texttt{vm\_regular\_engine}.

\subsection{Trace To IR}
Nash compiles traces to relaxed A-normal form~\cite{flanagan1993essence}
internal representation (IR) before assigning registers and assembling to
native code. Figure~\hyperref[fig:anf]{\ref{fig:anf}} shows IR of primitive
operations compiled from the recorded trace in
Figure~\hyperref[fig:trace]{\ref{fig:trace}}. The IR primitives contain two
\texttt{lambda} terms, the first block is for prologue, and the second block
is for loop body. The IR uses \texttt{let*} instead of \texttt{let} to express
the sequence of computations. Each primitive operation takes two arguments,
except for \texttt{\%snap} operation. Primitive operations updating a
variable, such as \texttt{\%add}, have the variable to be updated on the
left-hand side of the expression. Primitive operations without variable update
contain a symbol \texttt{\_} at the left-hand side. The variables starting
with the letter \texttt{v} indicates that the variable was loaded from current
stack. The variables starting with the letter \texttt{r} indicates that the
variable is for temporal use only.

\subsubsection{Snapshot}

\begin{figure*}
  \centering
  \small
\begin{verbatim}
7f4ddb7fc574  (uadd/immediate 0 6 1)            ; #(#x3e #x2a2 #x1 #x0 #x3e8 #x7f4ddb808660 #x3e)
7f4ddb7fc578  (vector-ref 6 5 6)                ; #(#x3f #x2a2 #x1 #x0 #x3e8 #x7f4ddb808660 #x3e)
7f4ddb7fc57c  (br-if-u64-<-scm 3 6 #t 4)        ; #(#x3f #x2a2 #x1 #x0 #x3e8 #x7f4ddb808660 #x1a)
7f4ddb7fc588  (add 1 1 6)                       ; #(#x3f #x2a2 #x1 #x0 #x3e8 #x7f4ddb808660 #x1a)
7f4ddb7fc58c  (br-if-u64-<= 4 0 #f 9)           ; #(#x3f #x2ba #x1 #x0 #x3e8 #x7f4ddb808660 #x1a)
7f4ddb7fc598  (mov 6 0)                         ; #(#x3f #x2ba #x1 #x0 #x3e8 #x7f4ddb808660 #x1a)
7f4ddb7fc59c  (br -10)                          ; #(#x3f #x2ba #x1 #x0 #x3e8 #x7f4ddb808660 #x3f)
\end{verbatim}
\caption{Bytecode instructions and stack values recorded with running
  \texttt{sumv-positive}.}
\label{fig:trace}
\end{figure*}

\begin{figure}
  \centering
  \small
\begin{verbatim}
     1	(lambda ()
     2    (let* ((_    (%snap 0))
     3           (v0   (%sref 0 #f))
     4           (v1   (%sref 1 1))
     5           (v3   (%sref 3 67108864))
     6           (v4   (%sref 4 67108864))
     7           (v5   (%sref 5 131072))
     8           (v6   (%sref 6 67108864)))
     9      (loop v0 v1 v3 v4 v5 v6)))
    10	(lambda (v0 v1 v3 v4 v5 v6)
    11    (let* ((v0   (%add v6 1))
    12           (_    (%snap 1 v0 v1 v6))
    13           (r2   (%cref v5 0))
    14           (r2   (%rsh r2 8))
    15           (_    (%lt v6 r2))
    16           (r2   (%add v6 1))
    17           (v6   (%cref v5 r2))
    18           (_    (%snap 2 v0 v1 v6))
    19           (_    (%typeq v6 1))
    20           (_    _)
    21           (r2   (%rsh v6 2))
    22           (_    (%lt v3 r2))
    23           (_    (%snap 3 v0 v1 v6))
    24           (v1   (%addov v1 v6))
    25           (v1   (%sub v1 2))
    26           (_    (%snap 4 v0 v1 v6))
    27           (_    (%gt v4 v0))
    28           (v6   v0))
    29      (loop v0 v1 v3 v4 v5 v6)))
\end{verbatim}
\caption{IR of recorded trace in relaxed A-normal form.}
\label{fig:anf}
\end{figure}

Between some recorded bytecode instructions, \texttt{\%snap} expressions are
inserted to make \textit{snapshot} data. Snapshots contain various information
to recover the state of \texttt{vm\_nash\_engine} when native code pass the
control back. Snapshot data contains local indices to store variables, and a
bytecode IP to tell \texttt{vm\_nash\_engine} where the interpretation should
continue. The expression \texttt{\%snap} takes variable number of arguments:
the first argument is a \textit{snapshot ID}, which is a unique integer number
to identify the snapshot in single trace. The rest of the arguments are local
variables to be stored in current stack. In
Figure~\hyperref[fig:anf]{\ref{fig:anf}}, Nash inserted \texttt{\%snap}
expression at the beginning, and before the primitive operations
\texttt{\%lt}, \texttt{\%typeq}, \texttt{\%addov}, and \texttt{\%gt}, which
act as guards. The primitives \texttt{\%lt} and \texttt{\%gt} does arithmetic
less-than and greater-than comparisons, respectively. The primitive
\texttt{\%typeq} does type check with given variable and type, and the
primitive \texttt{\%addov} does addition with overflow check. When the result
of guard differed from the result observed at the time of JIT compilation,
native code executes the recovering steps to setup the state in
\texttt{vm\_nash\_engine}, and the program continues with interpreting
bytecode instructions.


\subsubsection{Prologue section}
\label{sec:irprologue}

The prologue section, the first \texttt{lambda} block shown in
Figure~\hyperref[fig:anf]{\ref{fig:anf}}, loads initial values from the stack
with \texttt{\%sref} primitive. The first argument passed to \texttt{\%sref}
is a local index offset, the second argument is an integer representation of
the type of expected local in the stack. For instance, the value \texttt{1} is
for \texttt{fixnum}, which means small integer value in Scheme,
\texttt{131072} is vector object, \texttt{67108864} is \texttt{u64}, which is
an unsigned 64 bit integer value to alias \texttt{scm\_t\_uint64} type defined
in C, and so on.

%% Some of the type information are fully resolved by bytecode instruction. For
%% instance, argument types passed to \texttt{br-if-u64-<=} are two \texttt{u64},
%% which is determined at the time of Scheme source code to bytecode compilation.

%% Some of the bytecode instructions have polymorphic types, in a sense of Nash
%% compiler. In such case, the types are specialized from the stack values
%% recorded along the bytecode. The tc3 tag (described in
%% Section~\ref{sec:scmdatatype}) of each value is observed and a type check for
%% the locals are added as necessary. For instance, bytecode instruction
%% \texttt{add} could take fixnum, flonum, complex, and bignum values. The
%% \texttt{(add~1~1~6)} shown in Figure~\hyperref[fig:trace]{\ref{fig:trace}}
%% adds local 1 and local 6, stores the result to local 1. In the recorded stack
%% values, the local 1 (the second element of the vector) is \texttt{\#x2a2},
%% which is \texttt{1010100010} in binary. Nash could tell that this value is a
%% small integer since it has a tc3 tag \texttt{010}. Similarly, local 6 (the
%% seventh element of the vector) in the recorded stack is \texttt{\#1a}, which
%% is \texttt{11010} in binary, again, a small integer.

The value \texttt{\#f} in \texttt{\%sref} primitive means that there is no
need for type check, for instance the local is overwritten without
referencing. The variable \texttt{v0}, which hold local \texttt{0} in above
example is immediately overwritten by result of \texttt{\%add} primitive in
line 11 of Figure~\hyperref[fig:anf]{\ref{fig:anf}}. There is not need to load
this local from current stack, though such dead-code eliminations are not yet
implemented.

\subsubsection{Loop body section}

The loop body section, the second \texttt{lambda} block in
Figure~\hyperref[fig:anf]{\ref{fig:anf}}, is compiled by translating each
recorded bytecode instruction sequentially.

\paragraph{uadd/immediate} The first primitive operation contains
\texttt{\%add}, which does arithmetic addition with variable \texttt{v6} and
constant value $1$. The result of addition overwrites variable \texttt{v0}. No
overflow check is done with \texttt{uadd/immediate} in bytecode interpreters,
and result will wrap around. Native code followed this behavior.

\paragraph{vector-ref} Then a snapshot 1 is inserted, and
the primitive operations for \texttt{vector-ref} follows. The primitive
operations contain vector index range check, by comparing the length of vector
with the index value passed to \texttt{vector-ref} instruction. For Scheme
\texttt{vector} object, Guile uses the first one word to store a \textit{tc7}
tag and the length of the vector. A tc7 tag is, like tc3 tag, a 7 bits long
tag value used to distinguish types. The length is left shifted for 8 bits so
that the tc7 tag value and the length could fit in single word. Actual vector
elements are stored from the memory address of the SCM object plus one word.

The primitive operation \texttt{\%cref} in line 13 loads a value from Scheme
heap object with offset 0 and stores the loaded value to temporary register
\texttt{r2}. Then \texttt{r2} is passed to \texttt{\%rsh} in line 14, which
does arithmetic right shift for 8 bits and overwrite the value of
\texttt{r2}. Now \texttt{r2} contains the reproduced vector length, and
compared with \texttt{v6}, which is the variable holding local 6, which is the
index value used in recorded \texttt{vector-ref} instruction. Line 16 adds $1$
to \texttt{v6} to get the offset of vector element. Line 17 does another
\texttt{\%cref} to load the vector element, and overwrites the value of
\texttt{v6}.

\paragraph{br-if-u64-\texttt{<}-scm} Line 18 contains a snapshot used by next
primitive operation \texttt{\%typeq}, which does a type check of \texttt{v6}
with \texttt{fixnum}. The variable \texttt{v6} is an element in the argument
\texttt{vector}, which was observed as \texttt{fixnum} at the time of JIT
compilation. Line 20 shows empty value assigned to empty value. This line used
to contain a \texttt{\%snap} expression, though the JIT compiler has optimized
away the snapshot, since the bytecode IP destination in snapshot data was
identical with the previous snapshot, and no variables were updated by
\texttt{\%typeq}. Nash does few on-the-fly optimizations, such as this cached
snapshot reuse, duplicated guard elimination, and constant folding.  Variable
\texttt{v6} is right shifted for 2 bits to move away the tc3 tag of
\texttt{fixnum}, and the result is stored to variable \texttt{r2} in line 21,
to compare with variable \texttt{v3} in line 22. Bytecode instruction
\texttt{br-if-u64-<-scm} takes \texttt{u64} type as its first argument, SCM
type as its second argument, and compares the two. The type of variable
\texttt{v3} is determined as \texttt{u64} at the time of Scheme to bytecode
compilation, no type checks are done in native code.

\paragraph{add} Line 23 contains a snapshot, which will be used
when arithmetic overflow occurred. Line 24 adds two \texttt{fixnum} values in
\texttt{v1} and \texttt{v6} with \texttt{\%addov} primitive, and overwrites
the contents of \texttt{v1}. Type checks for \texttt{v1} and \texttt{v6} are
not done, since the \texttt{fixnum} type check done by \texttt{\%typeq} for
\texttt{v6} is still valid, and \texttt{fixnum} type check for \texttt{v1} is
already done in prologue section. Resulting type from addition of two
\texttt{fixnum} is again a \texttt{fixnum} unless it overflows, thus type
check for \texttt{v1} inside loop body is eliminated. The \texttt{\%sub}
primitive in line 25 subtracts the extra tc2 bits added by \texttt{\%addov}.

\paragraph{br-if-u64-\texttt{<}=} Line 26 inserts another snapshot. Then in
line 27, two 64 bit unsigned values in \texttt{v4} and \texttt{v0} are
compared. Types of the variables are determined by the bytecode.

\paragraph{mov} Line 28 simply does a move, and overwrites the contents of
\texttt{v6} with \texttt{v0}.

\paragraph{br} Then the IR shows a call to \texttt{loop}, which tells that the
computation jumps to the beginning of the loop body section. A loop body of
native code exits when any of the guards failed.  For instance, when the
result returned by \texttt{(\%gt~v4~v0)} differed from the result observed at
the time of JIT compilation, native code will pass the control back to
\texttt{vm\_nash\_engine}, recover the interpreter state by using snapshot
data from \texttt{(\%snap~3~v0~v1~v6)} and the bytecode interpretation of
input program will continue from bytecode IP \texttt{7f4ddb7fc5b0}, which is
the jump destination of \texttt{(br-if-u64-<=~4~0~\#f~9)} in recorded trace.
\footnote{ \texttt{7f4ddb7fc5b0 = 7f4ddb7fc58c + (9 * 4)}.
  \texttt{7f4ddb7fc58c} is the absolute bytecode IP of \texttt{(br-if-u64-<= 4
    0 \#f 9)} shown in Figure~\hyperref[fig:trace]{\ref{fig:trace}}, $9$ is
  the offset argument passed to \texttt{br-if-u64-<=}, and $4$ is the size of
  single byte used for bytecode.}

\begin{figure}
  \centering
  \small
\begin{verbatim}
----     [snap  0] ()
0001     (%sref    r14 +0 ---)
0002     (%sref    r15 +1 fixn)
0003     (%sref    r9 +3 u64)
0004     (%sref    r8 +4 u64)
0005     (%sref    rcx +5 vect)
0006     (%sref    rdx +6 u64)
==== loop:
0007     (%add     r14 rdx +1)
----     [snap  1] ((0 u64) (1 fixn) (6 u64))
0009     (%cref    r11 rcx +0)
0010     (%rsh     r11 r11 +8)
0011   > (%lt      rdx r11)
0012     (%add     r11 rdx +1)
0013     (%cref    rdx rcx r11)
----     [snap  2] ((0 u64) (1 fixn) (6 scm))
0015   > (%typeq   rdx fixn)
0016     (%rsh     r11 rdx +2)
0017   > (%lt      r9 r11)
----     [snap  3] ((0 u64) (1 fixn) (6 scm))
0019   > (%addov   r15 r15 rdx)
0020     (%sub     r15 r15 +2)
----     [snap  4] ((0 u64) (1 fixn) (6 scm))
0022   > (%gt      r8 r14)
0023     (%move    rdx r14)
\end{verbatim}
\caption{Primitive operation of recorded trace under x86-64
  architecture. Slightly modified from dumped output for displaying purpose.}
\label{fig:primops}
\end{figure}

\begin{figure}
  \centering
  \small
\begin{verbatim}
0x02cc005b mov    r14,QWORD PTR [rbx]
0x02cc005e mov    r15,QWORD PTR [rbx+0x8]
0x02cc0062 mov    r9,QWORD PTR [rbx+0x18]
0x02cc0066 mov    r8,QWORD PTR [rbx+0x20]
0x02cc006a mov    rcx,QWORD PTR [rbx+0x28]
0x02cc006e mov    rdx,QWORD PTR [rbx+0x30]
0x02cc0072 nop    WORD PTR [rax+rax*1+0x0]
loop:
0x02cc0078 lea    r14,[rdx+0x1]
0x02cc007c mov    r11,QWORD PTR [rcx]
0x02cc007f sar    r11,0x8
0x02cc0083 cmp    rdx,r11
0x02cc0086 jge    0x02ccc028    ->1
0x02cc008c lea    r11,[rdx+0x1]
0x02cc0090 lea    rax,[r11*8+0x0]
0x02cc0098 mov    rdx,QWORD PTR [rax+rcx*1]
0x02cc009c test   rdx,0x2
0x02cc00a3 je     0x02ccc030    ->2
0x02cc00a9 mov    r11,rdx
0x02cc00ac sar    r11,0x2
0x02cc00b0 cmp    r9,r11
0x02cc00b3 jge    0x02ccc030    ->2
0x02cc00b9 mov    r11,r15
0x02cc00bc add    r11,rdx
0x02cc00bf jo     0x02ccc038    ->3
0x02cc00c5 mov    r15,r11
0x02cc00c8 sub    r15,0x2
0x02cc00cc cmp    r8,r14
0x02cc00cf jle    0x02ccc040    ->4
0x02cc00d5 mov    rdx,r14
0x02cc00d8 jmp    0x02cc0078    ->loop
0x02cc00dd nop    DWORD PTR [rax]
\end{verbatim}
\caption{Native code compiled from trace, under x86-64 architecture.}
\label{fig:ncode}
\end{figure}

\subsection{IR To Native Code}

Figure~\hyperref[fig:primops]{\ref{fig:primops}} shows the IR in
Figure~\hyperref[fig:anf]{\ref{fig:anf}} after register assignment. The
numbers in left of each line, which are omitted for snapshot data, tells
primitive operation numbers. The variables in
Figure~\hyperref[fig:primops]{\ref{fig:primops}} are using register names for
x86-64 architecture. Nash has a module with architecture specific register
definitions. At the time of writing, supported architecture is x86-64 only.

Nash uses GNU lightning as an assembler backend. GNU lightning is a JIT
compilation library, which runs under various architectures including aarch64,
alpha, arm, ia64, mips, powerpc, s390, sparc, and x86. Nash contains a thin C
wrapper which binds SCM type to the types understood by GNU lightning. The
resulting bindings are called from Scheme code under
\texttt{vm\_regular\_engine} just like other Scheme procedures defined in C.

Figure~\hyperref[fig:ncode]{\ref{fig:ncode}} shows dumped x86-64 native code
of primitive operations in
Figure~\hyperref[fig:primops]{\ref{fig:primops}}. The native code contains a
mark to show the beginning of the loop body with \texttt{loop:}, and snapshot
ID numbers in lines with jump instruction.

Native code compiled in Nash contains debug symbols to interact with JIT
compilation interface of GDB~\cite{stallman2002debugging}, a well-known
open-source debugger. This debugging support is turned off by default, but can
be turned on with command line option when invoking \texttt{guile} executable.
\footnote{Other than debug symbol, Nash contains a command line option to dump
  intermediate data during compilation. Contents of
  Figure~\hyperref[fig:anf]{\ref{fig:trace}},
  \hyperref[fig:anf]{\ref{fig:anf}},
  \hyperref[fig:primops]{\ref{fig:primops}}, and
  \hyperref[fig:ncode]{\ref{fig:ncode}} are obtained from the dumped output.}

\section{Evaluation}
\label{sec:evaluation}

\begin{figure}
  \centering
  \includegraphics[angle=270,origin=c,width=0.5 \textwidth]{hist}
  \caption{Benchmark results of VM-Nash and Guile interpreter, total time
    normalized to VM-regular. Lower is better.}
  \label{fig:bench}
\end{figure}

\subsection{Settings}

Performance of Nash is evaluated with cross platform benchmark suite from
Pycket project. The source code of the benchmarks originate from Larceny and
Gambit project. Some modifications were made to the Pycket version. Benchmarks
\texttt{bv2string}, \texttt{ntakl} and \texttt{quicksort} were added, which
exist in the original Larceny and Gambit benchmark suite. Iteration counts for
\texttt{ctak}, \texttt{fft}, \texttt{pnpoly}, \texttt{fibc}, and \texttt{ray}
are decreased, which were taking long execution time in Guile's
VM-regular. The modified benchmark suite contains 57 programs. Total elapsed
time of each program including JIT warm up time was measured. The benchmark
results were taken under a machine with Intel Core-i5 3427-U and 8GB of
memory, running Arch Linux, Linux kernel 4.5.4.

\subsection{Results}
\label{sec:results}

\subsubsection{Comparison With VM-regular And Interpreter}
\label{sec:guilecomp}

\begin{table}
  \centering
  \begin{tabular}{rrr}
     & Nash & Interpreter \\
    \toprule
    sumfp & 0.024 & 7.926 \\
    mbrot & 0.034 & 6.626 \\
    sum & 0.119 & 42.579 \\
    sumloop & 0.157 & 54.323 \\
    \midrule
    sum1 & 0.905 & 1.232 \\
    pi & 0.999 & 1.503 \\
    string & 1.115 & 1.125 \\
    \midrule
    ctak & 1.073 & 2.243 \\
    fibc & 1.678 & 2.507 \\
    parsing & 1.654 & 9.685 \\
    dynamic & 2.506 & 6.359 \\
    \midrule
    GM & 0.400 & 13.770 \\
  \end{tabular}
  \caption{Selected benchmark results from
    Figure~\hyperref[fig:bench]{\ref{fig:bench}} and geometric mean of the
    benchmarks shown in numbers.}
\label{tab:compguile}
\end{table}

Figure~\ref{fig:bench} shows total times of benchmark results of Nash and
Guile's plain interpreter, normalized to Guile's VM-regular, in logarithmic
scale.  The Guile interpreter runs Scheme source code without bytecode
compilation.  Table~\hyperref[tab:compguile]{\ref{tab:compguile}} contains
selected benchmark results and geometric means of the benchmarks in numbers.

Nash achieved significant performance improvements in benchmark programs
containing tight loop with Scheme flonum values, such as \texttt{sumfp} and
\texttt{mbrot}.  Nash also achieved relatively large performance improvements
with programs containing tight loop without Scheme flonum values, such as
\texttt{sum} and \texttt{sumloop}.

Some of the benchmarks, such as \texttt{sum1}, \texttt{pi}, and
\texttt{string}, showed similar performance in Nash, VM-regular, and Guile
interpreter. In these benchmarks, large amount of the time were spent in
procedures written in C functions, such as \texttt{read},
\texttt{string-append}, and arithmetic procedures for large integer numbers.
All three implementations are calling the same underlying C functions, thus
the performance differences were relatively small.

In benchmarks with many continuation object creations, such as \texttt{ctak}
and \texttt{fibc}, Nash runs slower than VM regular. Nash implements
\texttt{call/cc} with C function to keep compatibility with C interface of
libguile, which did not lead to performance improvement.  Performance overhead
caused by JIT compilation and stack state recovery have slowed down the
overall performance.  Benchmarks \texttt{parsing} and \texttt{dynamic} did not
contain creation of continuation objects, though these benchmarks had large
amount of JIT compilation overhead caused by conditional branches.

\begin{figure}
  \centering
  \includegraphics[angle=270,origin=c,width=0.5 \textwidth]{dist}
  \caption{Distribution of benchmark results from native code compilers, and
    geometric standard scores of Nash and Pycket.  Outliers are not
    shown. Lower is better.}
  \label{fig:dist}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{lccp{11mm}l}
    Name (version) & GM & N & Failed & Strategy \\
    \toprule
    Chez (9.4.1) &  0.148 & 57 & & Incremental \\
    \midrule
    Bigloo (4.2c) & 0.236 & 54 & pi, trav1, trav2 & AOT via C \\
    \midrule
    Ikarus (0.0.4rc1) & 0.244 & 56 & parsing & Incremental \\
    \midrule
    Pycket (5f98bf3) & 0.252 & 57 & & Tracing JIT \\
    \midrule
    Gambit (4.8.5) & 0.274 & 56 & gcold & AOT via C \\
    \midrule
    Larceny (0.99) &  0.301 & 57 & & Incremental \\
    \midrule
    Racket  (6.6) &  0.324 & 57 & & Method JIT \\
    \midrule
    Nash & 0.400 & 57 & & Tracing JIT \\
    \midrule
    Chicken (4.11.0) & 0.448 & 54 & gcold, maze, pi & AOT via C \\
    \midrule
    MIT (9.2) &  0.486 & 55 & parsing, nucleic & AOT \\
  \end{tabular}
  \caption{Benchmarked Scheme native code compilers. Geometric means of
    benchmark results normalized to VM-regular, lower is better. Geometric
    means are calculated from valid results only, column \texttt{N} shows the
    number of valid results. Version for Pycket is git revision.}
\label{tab:nativecomp}
\end{table}

\subsubsection{Comparison With Native Code Compilers}

The benchmark results were taken with various Scheme native code compilers.
Table~\hyperref[tab:impls]{\ref{tab:nativecomp}} summarizes benchmark results
from native code compilers with geometric means computed from valid benchmark
results, number of succeeded benchmarks, and name of the failed
benchmarks. Scheme to C compilers used GCC version 6.1.1 to compile the
resulting C codes. Figure~\hyperref[fig:dist]{\ref{fig:dist}} shows boxplot of
benchmark results and geometric standard scores from Nash and Pycket. Table
\hyperref[tab:tracingjits]{\ref{tab:tracingjits}} shows selected benchmark
results in numbers.

\begin{table}
  \centering
  \input{tracingjits.inc}
  \caption{Selected benchmark results from
    Figure~\hyperref[fig:dist]{\ref{fig:dist}}. Showing geometric standard
    deviation, geometric standard score of Nash, and geometric standard score
    of Pycket. Lower standard score is better.}
\label{tab:tracingjits}
\end{table}

Implementations using tracing JIT had some characteristics in common. Both
Nash and Pycket performed well with benchmarks containing tight loops. In
\texttt{sumfp}, \texttt{mbrot}, \texttt{sum}, \texttt{array1} and
\texttt{sumloop}, Pycket and Nash are the fastest and second to the fastest
implementations.  In \texttt{matrix}, \texttt{peval} and \texttt{dynamic},
Nash and Pycket performed badly. These benchmarks contained lots of
conditional branches caused from data dependent control flows, which is a
known weakness of tracing JIT \citep{bauman2015pycket}.

Pycket performed well with benchmark containing extensive use of
\texttt{call/cc}, such as \texttt{fibc} and \texttt{ctak}. The geometric
standard deviation of \texttt{fibc} and \texttt{ctak} are relatively larger
than other benchmarks. The implementation of continuation differed across
native code compilers, which lead to wider range of benchmark results. As
mentioned in Section~\ref{sec:guilecomp}, the performance of Nash in the
benchmarks containing \texttt{call/cc} are slower than VM-regular.

In \texttt{fibfp}, \texttt{simplex}, and \texttt{nucleic}, Pycket was the
fastest implementation and Nash was the slowest implementation. All three of
the benchmarks contained Scheme flonum values, which were efficiently handled
in Pycket but not in Nash.  Nash performed badly in \texttt{parsing} benchmark
also, but the performance result from Pycket was the median value. Performance
improvement for \texttt{parsing} benchmark is a known problem for Nash which
requires further efforts.

%% \section{Discussion}

%% Approach for recording of trace is different from Pycket. Pycket had a
%% problem with \textit{cyclic-path}, but Nash didn't. Might worth explaining
%% the natural-loop-first (NLF) in LuaJIT.

\section{Limitations and Future Work}
\label{sec:future}

Nash is still an experimental implementation, has a lot of space for
improvement. Some of the possibilities for future works follows.

\paragraph{Support more bytecode instructions} Nash still have bytecode
instructions which are not implemented at all, such as \texttt{prompt} and
\texttt{abort} used for delimited continuation, or partially implemented, such
as arithmetic operations. Arithmetic operations for multi-precision numbers
and complex numbers are not yet implemented. When JIT compiler encountered
unsupported bytecode, it aborts the work and falls back to
\texttt{vm\_nash\_engine}.

\paragraph{Detect more loops} Nash detects loops with backward jump,
tail-calls, and consequent calls for non-tail-call recursion
(\textit{down-recursion}\footnote{Consequent calls to procedure are called as
  down-recursion in Nash, because Guile's stack grows down}), though not with
consequent returns from non-tail-call recursions (\textit{up-recursion}) yet.

%% Nash does not detect \textit{looping side traces}. Side trace start from
%% frequently taken exit in native code. Side traces are usually patched to
%% existing trace when \texttt{vm\_nash\_engine} encountered beginning
%% bytecode IP of existing trace. In looping side trace, bytecode instructions
%% loops inside the side trace instead of patched back.

\paragraph{Optimize IR} Nash does only a few IR level
optimizations. In prologue section of IR shown in
Section~\hyperref[sec:irprologue]{\ref{sec:irprologue}}, unnecessary load from
stack exist, which could be omitted by dead-code elimination. More optimizations
could be done, such as loop invariant code motion, escape analysis, allocation
removals, and so on. Also, Nash currently uses naive method to assign
registers. More sophisticated method such as Linear-Scan register
allocation~\cite{poletto1999linear} could be used.

\paragraph{Blacklist} Benchmarks such as \texttt{parsing} and
\texttt{dynamic} were slower than Guile VM-regular. Programs containing large
number of branching conditions need some treatments to perform well under
tracing JIT.\@ One approach is to limit the JIT compilation when branching
exceed certain threshold. These thresholds mechanism are sometime called
\textit{blacklisting} of trace. Nash could take more sophisticated approach to
blacklist unwanted traces.

\section{Related Work}
\label{sec:related}
Implementation of Nash was inspired from pioneers in tracing JIT field. Nash
does type checks in VM interpreter before running compiled native code, which
was done in TraceMonkey~\cite{gal2009trace} with similar design.

The mechanism in interpreter to record instructions and detect hot loops are
inspired from Pypy and RPython~\cite{bolz2009tracing}. The way how Nash mark
\texttt{NEXT} and loop starting bytecode definitions are influenced by the
approach used in interpreter written with RPython.  RPython provides a
framework to define a new language by writing an interpreter, while Guile
provides a framework to define a new language by writing a compiler.  Use of
A-normal form~\cite{flanagan1993essence} was inspired from
Pycket~\cite{bauman2015pycket}. Though Pycket expands the source code and
generates JSON data, and parses it to AST for execution. Both Nash and Pycket
use tracing JIT in its implementation. Results shown in
Section~\ref{sec:evaluation} proved that Pycket is generally faster than Nash.
However, Pycket is designed as a batch file compiler in single threaded
computation. As of the version used in the benchmarks, Pycket lacks REPL,
threads, and FFI support.

Design of snapshot was inspired from LuaJIT~\cite{pall2009ip}. LuaJIT uses
more sophisticated approach and compressed snapshot data. LuaJIT used
NaN-tagging, which enables efficient handling of unboxed floating point
numbers. Guile once had an attempt to use NaN-tagging~\cite{wingo2011value},
though the attempt wasn't merged to Guile source code, due to supporting
conservative garbage collector under 32 bit architectures.

\section{Conclusion}
\label{sec:conclusion}
This paper has shown how Nash is designed. Nash reused existing bytecode
interpreter in Guile, turned it to a trace recording interpreter with few
small modifications. Nash coexist with existing VM, which is used for JIT
compiler written in Scheme. Existing bytecode compiled by Guile are traced,
recorded and compiled to native code. Performance comparison between Nash,
Guile's existing VM, and various Scheme compilers were done with 57
benchmarks. With keeping itself as an extension language, Nash showed
significant speed ups in programs with tight loops, and achieved competitive
speed with Scheme implementations with native code compiler in overall
performance.

%% \appendix
%% \section{Appendix: title}
%% This is the text of the appendix, if you need one.

%% \acks{} Acknowledgments, if needed.
\acks{} Nash exists because of the hard works done for GNU Guile by many open
source developers.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

%% \begin{thebibliography}{}
%% \softraggedright{}
%% ...
%% \end{thebibliography}

% Using .bib file at the moment.
\bibliography{nash}

\end{document}

%%  LocalWords: AOT Pycket ANF TinyScheme Gauch Ypsilon SCM disassembler tc VM
%%  LocalWords: typedef Immediates immediates LuaJIT Pypy Lua Rpython fixnum
%%  LocalWords: RPython backend sparc powerpc mips aarch ia TraceMonkey JSON
%%  LocalWords: AST unboxed NaN bytecode JIT TCALL untagging REPL

%% Local Variables:
%% fill-column: 78
%% End:
